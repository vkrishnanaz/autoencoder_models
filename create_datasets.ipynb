{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed61d8e4-3200-4bff-93ec-a714678c40ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e451d9-daf2-4e27-aab8-7b4fd1a1c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(particles, bmin, bmax, emin, emax):\n",
    "    particles_n = np.ones([particles.shape[0], particles.shape[1], 4])\n",
    "    idx_pt, idx_eta, idx_phi, idx_class = range(4)\n",
    "    pt = particles[:,:,0]\n",
    "    \n",
    "    pt_norm = pt#pt/pt.sum(axis=1, keepdims=True)#(pt - bmin) / (bmax-bmin)\n",
    "\n",
    "\n",
    "\n",
    "    #pt_norm = (pt - np.min(pt, axis = 1, keepdims=True)) / (np.max(pt, axis = 1, keepdims=True)-np.min(pt, axis=1, keepdims=True))\n",
    "\n",
    "    particles_n[:,:,0] = pt_norm/pt.sum(axis=1, keepdims=True)#pt_norm*np.cos(particles[:,:,idx_phi])\n",
    "    particles_n[:,:,1] = particles[:,:,idx_eta]#pt_norm*np.sin(particles[:,:,idx_phi])\n",
    "    particles_n[:,:,2] = particles[:,:,idx_phi]#np.tanh(particles[:,:,idx_eta]) #(particles[:,:,idx_eta]-emin)/(emax-emin)#particles[:,:,idx_eta]#pt_norm*np.sinh(particles[:,:,idx_eta])\n",
    "\n",
    "    particles_n[:,:,idx_class] = particles[:,:,idx_class]#(particles[:,:,idx_class] - np.min(particles[:,:,idx_class])) / (np.max(particles[:,:,idx_class])-np.min(particles[:,:,idx_class]))\n",
    "\n",
    "    return particles_n    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a4d0e6-4799-459e-ba0b-fb407f194709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_convolutional(bkg_file, output_bkg_name, signals_files, output_signal_names, events=None,\n",
    "                                  test_size=0.4, val_size=0.1, input_shape=57):\n",
    "    # read BACKGROUND data\n",
    "    with h5py.File(bkg_file, 'r') as file:\n",
    "        full_data = file['Particles'][:, :, :]\n",
    "        print(full_data[0])\n",
    "        np.random.shuffle(full_data)\n",
    "        if events: full_data = full_data[:events, :, :]\n",
    "    bmin = np.min(full_data[:,:,0])\n",
    "    bmax = np.max(full_data[:,:,0])\n",
    "    emin = np.min(full_data[:,:,2])\n",
    "    emax = np.max(full_data[:,:,2])\n",
    "    \n",
    "    full_data = normalize_features(full_data, bmin, bmax, emin, emax)[:, :, :-1]\n",
    "\n",
    "\n",
    "    # define training, test and validation datasets\n",
    "    X_train, X_test = train_test_split(full_data, test_size=test_size, shuffle=True)\n",
    "    X_train, X_val = train_test_split(X_train, test_size=val_size)\n",
    "\n",
    "    X_train = np.reshape(X_train, (-1, 19, 3, 1))\n",
    "    X_test = np.reshape(X_test, (-1, 19, 3, 1))\n",
    "    X_val = np.reshape(X_val, (-1, 19, 3, 1))\n",
    "\n",
    "    del full_data\n",
    "\n",
    "    with h5py.File(output_bkg_name + '_dataset.h5', 'w') as h5f:\n",
    "        h5f.create_dataset('X_train', data=X_train)\n",
    "        h5f.create_dataset('X_test', data=X_test)\n",
    "        h5f.create_dataset('X_val', data=X_val)               \n",
    "\n",
    "    if signals_files:\n",
    "        # read SIGNAL data\n",
    "        for i, signal_file in enumerate(signals_files):\n",
    "            f = h5py.File(signal_file, 'r')\n",
    "            signal_data = np.reshape(normalize_features(f['Particles'][:,:,:], bmin, bmax, emin, emax)[:, :, :-1], (-1, 19, 3, 1))\n",
    "            with h5py.File(output_signal_names[i] + '_dataset.h5', 'w') as h5f2:\n",
    "                h5f2.create_dataset('Data', data=signal_data)\n",
    "        \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c72f67-d122-4fd3-b71a-6efa5d1daeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "[[27.41296387  0.         -2.0499022   1.        ]\n",
      " [24.20996857  1.63350999 -0.10349621  2.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"Start\")\n",
    "    signals_files = [\"signals/Ato4l_lepFilter_13TeV_filtered.h5\", \"signals/hChToTauNu_13TeV_PU20_filtered.h5\",\n",
    "                     \"signals/hToTauTau_13TeV_PU20_filtered.h5\",\n",
    "                     \"signals/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5\"]\n",
    "    \n",
    "    signal_names = [\"signals/Ato4l\", \"signals/hChToTauNu\", \"signals/hToTauTau\", \"signals/LQtoBTau\"]\n",
    "    \n",
    "    create_datasets_convolutional('signals/background_for_training.h5', 'signals/convolutional', signals_files, signal_names)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9a3f0-1964-4b03-a23e-64662bc14062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
